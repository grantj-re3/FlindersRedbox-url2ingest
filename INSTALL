1. APPLICATION ENVIRONMENT

This script has been tested in the following environment:
- GNU bash, version 4.1.2(1)-release (x86_64-redhat-linux-gnu)
- Red Hat Enterprise Linux Server release 6.3 (Santiago)
- 2.6.32-279.11.1.el6.x86_64 #1 SMP Sat Sep 22 07:10:26 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux
- https://github.com/redbox-mint/redbox-build-dev-handle/tree/redbox-handle-curation-demo-1.5.1
- https://github.com/redbox-mint/mint-build-dev-handle/tree/mint-handle-curation-demo-1.5.1

It is expected that the script will also operate with little or no modification
under other versions of bash, sh and ksh.

This software has not been designed to run under the Microsoft Windows
operating system, but it may be feasible under Cygwin (see http://www.cygwin.com).


2. INSTALLATION


2.1 WARNING

This script contains a cleanup() function which deletes a list of files defined
in shell variables. Hence you should use care when assigning/changing shell
variables. In addition you should consider the following to minimise the risk
of issues.
- Read all the documentation.
- Limit your changes to variables marked with the word 'CUSTOMISE' unless
  you know what you are doing.
- After configuration, run the application with the --dump option to show
  the configuration is as you expected.
- Consider running with the DRY_RUN variable set initially.


2.2 ASSUMPTIONS

- That you have a basic understanding of Linux/Unix and bash, sh or ksh shell
  scripts.
- That you are using a Linux, Unix or Unix-like environment similar to that
  described under APPLICATION ENVIRONMENT.
- That you are running the script as an unprivileged user (ie. not root).
- That the user who runs the script has permission to ingest (or harvest or
  load) Mint person and/or project files using the Mint server/tf_harvest.sh
  script.
- That person and/or project files to be ingested into Mint are CSV files
  compatible with the corresponding json file in the Mint home/harvest
  directory.

For loading party-person metadata, the script assumes there is a symlink
pointing from Mint home/data/Parties_People.csv to $TARGET_SOURCE_PATH.
This is important because Mint assumes the same ID field (ie. key)
in a different filename is a different record. Hence the symlink
allows us to potentially vary $TARGET_SOURCE_PATH filename while
still retaining the same "fileLocation" in home/harvest/Parties_People.json.
For loading activity-project metadata, the same principle applies to CSV
and JSON files.


2.3 INSTRUCTIONS

- Download from github.

- Checkout the desired version/tag if applicable.

- Update the config for your site
  * Update all variables which are commented with "CUSTOMISE" in the first
    section of the url2ingest.sh with values suitable for your environment.
  * For people and projects, add a symlink from the appropriate CSV file to
    $PERSON_FILTERED_FPATH or $PROJECT_FILTERED_FPATH respectively.
    Running the application with --dump option will show you the value of
    these shell variables.
  * For people and projects, if you want to apply a (regular expression)
    filter which is more restrictive than the default (which lets all records
    through) then update files etc/include_filter_people.conf and
    etc/include_filter_project.conf respectively. It is expected that filters
    are likely to be applied to either initially ingest a subset of records
    into Mint (during testing) or to apply sanity rules to records. Some
    further documentation and samples can be found in the etc/samples directory.

- The first time you run this script (or the first time you run the script
  after changing the CSV header) for people or projects, you must not use
  the --incremental-load option as there will be no reference CSV file from
  which to determine which records are new/updated.

- Subsequent runs can use either --full-load (default) or --incremental-load
  methods. Incremental loading is a little more risky because if the load
  fails for some reason, you may miss loading any new/updated records - even
  later on once the cause of the failure is resolved. However non-incremental
  (ie. full) loading is more likely to load those records later once the
  cause of the failure has been resolved.

- After running from the command line for testing and evaluation, you can
  configure the script to run for persons and/or projects from a single cron job.

See the EXAMPLE INSTALLATION SCENARIO below.


3. EXAMPLES


3.1 EXAMPLE INVOCATIONS

Command line help.
All the invocations below perform the same operation.
  ./url2ingest.sh --help
  ./url2ingest.sh -h

Command line invocation to show some variables in your configuration.
All the invocations below perform the same operation.
  ./url2ingest.sh --dump
  ./url2ingest.sh -d

Command line invocation to ingest all person records within a CSV file.
All the invocations below perform the same operation.
  ./url2ingest.sh --persons
  ./url2ingest.sh --persons --full-load
  ./url2ingest.sh -e
  ./url2ingest.sh -e -f

After priming with a full load, subsequent ingests can be incremental.
  ./url2ingest.sh --persons --incremental-load
  ./url2ingest.sh -e -i

Command line invocation to ingest all project records within a CSV file:
  ./url2ingest.sh --projects
  ./url2ingest.sh --projects --full-load
  ./url2ingest.sh -r
  ./url2ingest.sh -r -f

After priming with a full load, subsequent ingests can be incremental.
  ./url2ingest.sh --projects --incremental-load
  ./url2ingest.sh -r -i


Cron job to ingest all person records within a CSV file.
  15 20 * * * $HOME/opt/url2ingest/bin/url2ingest.sh --persons  >> $HOME/opt/url2ingest/log/url2ingest.log 2>&1

Cron job to ingest all project records within a CSV file.
  15 20 * * * $HOME/opt/url2ingest/bin/url2ingest.sh --projects >> $HOME/opt/url2ingest/log/url2ingest.log 2>&1

Cron job to ingest all person records within a CSV file then
all project records within a CSV file.
  15 20 * * * (app=$HOME/opt/url2ingest/bin/url2ingest.sh; $app --persons; $app --projects) >> $HOME/opt/url2ingest/log/url2ingest.log 2>&1


3.2 EXAMPLE INSTALLATION SCENARIO

Scenario:

 - This application to be installed in $HOME/opt/url2ingest of an unprivileged user.
 - Configure the ingest of person records from http://my_host.my_uni.edu.au/path/to/my_people.csv
 - Mint base is at /opt/ands/mint-builds/current.
 - /opt/ands/mint-builds/current/home/harvest/Parties_People.json contains the line:
     "fileLocation": "${fascinator.home}/data/Parties_People.csv",

	 
Instructions:

mkdir ~/opt
git clone https://github.com/grantj-re3/FlindersRedbox-url2ingest.git ~/opt/url2ingest
cd ~/opt/url2ingest

# If you want a particular release:
git tag			# List tagged releases
git checkout v1.1	# Checkout the desired release

# Read files INSTALL LICENSE README* etc/samples/*

# Edit config section of bin/url2ingest.sh; update any variable with the
# comment "## CUSTOMISE" to suit your site. Eg.

  MINT_BASE=/opt/ands/mint-builds/current
  PARENT_DIR=$HOME/opt/url2ingest
  PERSON_IN_URL_CSV=http://my_host.my_uni.edu.au/path/to/my_people.csv
  PERSON_MINT_DATA_SOURCE=Parties_People

# Add a symlink from CSV file location (associated with Mint
# Parties_People.json) to $PERSON_FILTERED_FPATH. Note that
# $PERSON_FILTERED_FPATH only exists while the script is running.

bin/url2ingest.sh --dump	# Show the value of $PERSON_FILTERED_FPATH
ln -s ~/opt/url2ingest/working/filtered_projects.csv /opt/ands/mint-builds/current/home/data/Parties_People.csv

# Consider if you want to apply more restrictive filtering to
# etc/include_filter_people.conf. The default regular expression (of .*)
# will match all records (including the CSV header record) hence all
# records will be ingested into Mint.
cat etc/include_filter_people.conf

# See if it runs ok
bin/url2ingest.sh --persons

# Check that records were ingested from the CSV file at the specified URL
# into the Mint Parties_People data source.
# Check log files: log/harvest_out_accum.log (and log/url2ingest.log if you
# redirected stdout and stderr to this file as per cron example)

# After successfully running the script your filesystem layout should look like that below.
$HOME/opt/url2ingest			# Parent directory.
$HOME/opt/url2ingest/bin		# Directory containing the script(s).
$HOME/opt/url2ingest/download		# Download directory (generally contains no files unless script is running).
$HOME/opt/url2ingest/download/history	# Download history directory. Contains (full) files downloaded from URL(s)
					#   and incremental file versions if applicable. The most recent timestamped
					#   full-file is used to determine next incremental version.
$HOME/opt/url2ingest/etc		# Contains configuration files.
$HOME/opt/url2ingest/log		# Log directory.
$HOME/opt/url2ingest/working		# Working/temporary directory (generally contains no files unless script is running).

# Add a line to your crontab similar to that below and confirm that it runs ok.
15 20 * * * $HOME/opt/url2ingest/bin/url2ingest.sh --persons  >> $HOME/opt/url2ingest/log/url2ingest.log 2>&1

# Celebrate!

