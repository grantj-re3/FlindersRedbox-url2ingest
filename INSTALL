1. APPLICATION ENVIRONMENT

This script has been tested in the following environment:
- GNU bash, version 4.1.2(1)-release (x86_64-redhat-linux-gnu)
- Red Hat Enterprise Linux Server release 6.3 (Santiago)
- 2.6.32-279.11.1.el6.x86_64 #1 SMP Sat Sep 22 07:10:26 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux
- https://github.com/redbox-mint/redbox-build-dev-handle/tree/redbox-handle-curation-demo-1.5.1
- https://github.com/redbox-mint/mint-build-dev-handle/tree/mint-handle-curation-demo-1.5.1

It is expected that the script will also operate with little or no modification
under other versions of bash, sh and ksh.

This software has not been designed to run under the Microsoft Windows
operating system, but it may be feasible under Cygwin (see http://www.cygwin.com).


2. INSTALLATION

2.1 WARNINGS

2.1.1 This script contains a cleanup() function which deletes a list of files
defined in shell variables. Hence you should use care when assigning/changing
shell variables.

2.1.2 Consider the following suggestions to minimise the risk of problems.
- Read all the documentation.
- Limit your changes to variables marked with the word 'CUSTOMISE' unless
  you know what you are doing.
- After configuration, run the application with the --dump option to show
  the configuration is as you expected.
- Consider running with the DRY_RUN variable set initially.

2.1.3 The etc/include_filter_*.conf files are used as a pattern-file in
an 'egrep -f ...' command. It appears that the GNU egrep command treats
blank lines in a pattern-file as a match-everything regular expression!
Hence you should NOT USE BLANK LINES anywhere within
etc/include_filter_*.conf files.


2.2 ASSUMPTIONS

- That you have a basic understanding of Linux/Unix and bash, sh or ksh shell
  scripts.
- That you are using a Linux, Unix or Unix-like environment similar to that
  described under APPLICATION ENVIRONMENT.
- That you are running the script as an unprivileged user (ie. not root).
- That the user who runs the script has permission to ingest (or harvest or
  load) Mint person and/or project files using the Mint server/tf_harvest.sh
  script.
- That person and/or project files to be ingested into Mint are CSV files
  compatible with the corresponding json file in the Mint home/harvest
  directory.

For loading party-person metadata, the script assumes there is a symlink
pointing from Mint home/data/Parties_People.csv to $FINAL_FPATH.
This is important because Mint assumes the same ID field (ie. key)
in a different filename is a different record. Hence the symlink
allows us to potentially vary $FINAL_FPATH filename while
still retaining the same "fileLocation" in home/harvest/Parties_People.json.

For loading activity-project metadata, the same principle applies to CSV
and JSON files.

After applying your customisations, you can find the value of $FINAL_FPATH
(and other interesting variables) for both persons and projects by running:
  url2ingest.sh --dump

For loading activity-project metadata, the same principle applies to CSV
and JSON files.


2.3 INSTRUCTIONS

- Download from github.

- Checkout the desired version/tag if applicable.

- Update the config for your site
  * Update all variables which are commented with "CUSTOMISE" in the first
    section of the url2ingest.sh with values suitable for your environment.
  * Add a symlink from the appropriate CSV file to the appropriate
    $FINAL_FPATH (for persons or projects).  Running the application with
    the --dump option will show you the value of these shell variables.
  * For people and projects, if you want to apply a (regular expression)
    filter which is more restrictive than the default (which lets all records
    through) then update files etc/include_filter_people.conf and
    etc/include_filter_project.conf respectively. It is expected that filters
    are likely to be applied to either initially ingest a subset of records
    into Mint (during testing) or to apply sanity rules to records. Some
    further documentation and samples can be found in the etc/samples directory.

- The first time you run this script (or the first time you run the script
  after changing the CSV header) for people or projects, you must not use
  the --incremental-load option as there will be no reference CSV file from
  which to determine which records are new/updated.

- Subsequent runs can use either --full-load (default) or --incremental-load
  methods. Incremental loading is a little more risky because if the load
  fails for some reason, you may miss loading any new/updated records - even
  later on once the cause of the failure is resolved. However non-incremental
  (ie. full) loading is more likely to load those records later once the
  cause of the failure has been resolved. However a full load for thousands
  of records is likely to cause a high processing load for Mint during the
  CSV file ingest and the NLA OAI-PMH incremental harvester (because all
  datestamps will be updated).

- After running from the command line for testing and evaluation, you can
  configure the script to run for persons and/or projects from a single cron job.

See the EXAMPLE INSTALLATION SCENARIO below.


3. EXAMPLES


3.1 EXAMPLE INVOCATIONS

Command line help.
All the invocations below perform the same operation.
  ./url2ingest.sh --help
  ./url2ingest.sh -h

Command line invocation to show some variables in your configuration.
All the invocations below perform the same operation.
  ./url2ingest.sh --dump
  ./url2ingest.sh -d

Command line invocation to ingest all person records within a CSV file.
All the invocations below perform the same operation.
  ./url2ingest.sh --persons
  ./url2ingest.sh --persons --full-load
  ./url2ingest.sh -e
  ./url2ingest.sh -e -f

After priming with a full load, subsequent ingests can be incremental.
  ./url2ingest.sh --persons --incremental-load
  ./url2ingest.sh -e -i

Command line invocation to ingest all project records within a CSV file:
  ./url2ingest.sh --projects
  ./url2ingest.sh --projects --full-load
  ./url2ingest.sh -r
  ./url2ingest.sh -r -f

After priming with a full load, subsequent ingests can be incremental.
  ./url2ingest.sh --projects --incremental-load
  ./url2ingest.sh -r -i


Cron job to ingest all person records within a CSV file.
  15 20 * * * $HOME/opt/url2ingest/bin/url2ingest.sh --persons  >> $HOME/opt/url2ingest/log/url2ingest.log 2>&1

Cron job to ingest all project records within a CSV file.
  15 20 * * * $HOME/opt/url2ingest/bin/url2ingest.sh --projects >> $HOME/opt/url2ingest/log/url2ingest.log 2>&1

Cron job to ingest all person records within a CSV file then
all project records within a CSV file.
  15 20 * * * (app=$HOME/opt/url2ingest/bin/url2ingest.sh; $app --persons; $app --projects) >> $HOME/opt/url2ingest/log/url2ingest.log 2>&1


3.2 EXAMPLE INSTALLATION SCENARIO

Scenario:

 - This application to be installed in $HOME/opt/url2ingest of an unprivileged user.
 - Configure the ingest of person records from http://my_host.my_uni.edu.au/path/to/my_people.csv
 - Mint base is at /opt/ands/mint-builds/current.
 - /opt/ands/mint-builds/current/home/harvest/Parties_People.json contains the line:
     "fileLocation": "${fascinator.home}/data/Parties_People.csv",

	 
Instructions:

mkdir ~/opt
git clone https://github.com/grantj-re3/FlindersRedbox-url2ingest.git ~/opt/url2ingest
cd ~/opt/url2ingest

# If you want a particular release:
git tag			# List tagged releases
git checkout v1.2	# Checkout the desired release

# Read files INSTALL, LICENSE, README*, RELEASE_NOTES, etc/samples/*

# Edit config section of bin/url2ingest.sh; update any variable with the
# comment "## CUSTOMISE" to suit your site. Eg.

  MINT_BASE=/opt/ands/mint-builds/current
  PARENT_DIR=$HOME/opt/url2ingest
  PERSON_IN_URL_CSV=http://my_host.my_uni.edu.au/path/to/my_people.csv
  PERSON_MINT_DATA_SOURCE=Parties_People

# Add a symlink from CSV file location (associated with Mint
# Parties_People.json) to the person $FINAL_FPATH. Note that
# $FINAL_FPATH only exists while the script is running.

bin/url2ingest.sh --dump	# Show the value of $FINAL_FPATH
ln -s ~/opt/url2ingest/working/final_people.csv /opt/ands/mint-builds/current/home/data/Parties_People.csv

# Consider if you want to apply more restrictive filtering to
# etc/include_filter_people.conf. The default regular expression (of .*)
# will match all records (including the CSV header record) hence all
# records will be ingested into Mint.
cat etc/include_filter_people.conf

# See if it runs ok
bin/url2ingest.sh --persons

# Check that records were ingested from the CSV file at the specified URL
# into the Mint Parties_People data source.
# Check log files: log/harvest_out_accum.log (and log/url2ingest.log if you
# redirected stdout and stderr to this file as per cron example)

# After successfully running the script your filesystem layout should look like that below.
$HOME/opt/url2ingest			# Parent directory.
$HOME/opt/url2ingest/bin		# Directory containing the script(s).
$HOME/opt/url2ingest/download		# Download directory (generally contains no files unless script is running).
$HOME/opt/url2ingest/download/history	# History directory. Contains CSV files in various stages of processing.
					#   The most recent timestamped filtered-file is used to determine next
					#   incremental CSV to be loaded (if applicable).
$HOME/opt/url2ingest/etc		# Contains configuration files.
$HOME/opt/url2ingest/log		# Log directory.
$HOME/opt/url2ingest/working		# Working/temporary directory (generally contains no files unless script is running).

# Add a line to your crontab similar to that below and confirm that it runs ok.
15 20 * * * $HOME/opt/url2ingest/bin/url2ingest.sh --persons  >> $HOME/opt/url2ingest/log/url2ingest.log 2>&1

# Celebrate!

